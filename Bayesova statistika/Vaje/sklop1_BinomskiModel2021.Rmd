---
title: '1. sklop: Binomski model'
author: "Nina Ruzic Gorenjec"
fontsize: 12pt
output:
  pdf_document:
    number_sections: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, fig.width = 8, fig.height = 5, out.width = "0.8\\textwidth")
```

\vspace{-1cm}
# Primer

Izberite pravilni odgovor na spodnje vprasanje.

Vprasanje: Qskd senciljm dowdlq a?

(a) 25

(b) 625

(c) 1

(d) Nic od nastetega.

**Zanima nas verjetnost, da odgovorimo pravilno.**

# Verjetnostni model za nas primer

Vzorec $X_1,X_2,\ldots,X_n$, kjer je:

* $n$ stevilo studentov na vajah,
  \vspace{0.1cm}
* $X_i$ predstavlja pravilnost odgovora $i$-tega studenta, tj. $X_i=1$, ce $i$-ti student odgovori pravilno, in $X_i=0$, ce le-ta odgovori napacno.

Preucujemo $X_1+X_2+\ldots+X_n$, tj. stevilo vseh pravilnih odgovorov, ki ga oznacimo z $X$ (druga standardna oznaka je $Y$ v smislu izida, anglesko *outcome*).

* $X\mid\theta \sim \text{Bin}(n,\theta)$
  \vspace{0.1cm}
* $P(X=k \mid \theta) = {n\choose k}\theta^{k}(1-\theta)^{n-k}$
  \vspace{0.1cm}
* $\theta$ (na predavanjih $\vartheta$) je verjetnost pravilnega odgovora -- **parameter, ki nas zanima**
  \vspace{0.1cm}
* $\text{E}(X)=n\theta$

Nas primer:
```{r}
n <- 26
```

Nasi podatki (oznacimo s $k$ realizacijo $X$ na nasem vzorcu):
```{r}
k <- 6
```

## Kako bi ocenili nas parameter s "klasicno" frekventisticno statistiko? Katere metode bi lahko uporabili?

\vspace{5cm}

## Bayesova formula

Na ravni dogodkov:
$$P(A \mid B) = \frac{P(B \mid A) \; P(A)}{P(B)} \propto P(B \mid A) \; P(A).$$
Na ravni gostot z enotno oznako $p$ (lahko bi pisali tudi $f$):
$$p(\theta \mid \text{podatki}) = \frac{p(\text{podatki} \mid \theta) \; p(\theta)}{p(\text{podatki})} \propto p(\text{podatki} \mid \theta) \; p(\theta).$$

V "standardnih" oznakah:
$$\pi(\theta \mid x) = \frac{f(x \mid \theta) \; \pi(\theta)}{f(x)} \propto f(x \mid \theta) \; \pi(\theta).$$
Podatki $x$ so znani. Zanima nas $\theta$. **Poglejmo zato na zgornje kot na funkcijo $\theta$.**

Spomnimo se funkcije verjetja (anglesko *likelihood*) $L(\theta \mid x) = L(\theta \,; x)$ in zapisimo zgornje kot:

$$\pi(\theta \mid x) \propto L(\theta \mid x) \; \pi(\theta).$$
**Trije gradniki Bayesove formule**, ki jih bomo predstavili graficno kot funkcije $\theta$:

* **Apriorna porazdelitev** $\pi(\theta)$ (njen integral je 1) -- *nase vnaprejsnje (apriorno) vedenje/prepricanje o $\theta$, preden zberemo podatke!*
  \vspace{0.1cm}
* **Verjetje** $L(\theta \mid x)$ (potrebno mnoziti s konstanto, tako da bo integral enak 1) -- *verjetnost podatkov pri razlicnih moznih vrednostih parametra $\theta$*.
  \vspace{0.1cm}
* **Aposteriorna porazdelitev** $\pi(\theta \mid x)$ -- *preko Bayesove formule posodobimo nase apriorno vedenje o parametru* ($\pi(\theta)$) *s tem, kar nam povedo podatki* ($L(\theta \mid x)$).

\newpage

## Verjetje

Stevilo pravilnih odgovorov med $n$ studenti je enako $k$:
$$L(\theta \mid x) = \theta^{k} \; (1-\theta)^{n-k}.$$

V R-u:
```{r}
verjetje <- function(theta, k, n){
  dbinom(k, size = n, prob = theta)
}

#Z mnozenjem s konst dosezemo, da je integral verjetja glede na theta enak 1.
konst <- function(k, n){
  theta <- seq(0.001, 1, 0.001)
  1 / (0.001 * sum(verjetje(theta, k, n)))
}
```

Narisemo za nas vzorec:
```{r}
theta <- seq(0, 1, 0.001)
konst.verjetje <- konst(k, n) * verjetje(theta, k, n)
plot(theta, konst.verjetje, type = "l", 
     xlab = expression(theta), ylab ="")
legend("topright", legend = c("verjetje"), col = c("black"),
       lty = 1, bty = "n", cex = 1.3)
```

\newpage

## Apriorna porazdelitev

Za apriorno porazdelitev si izberemo beta porazdelitev, ki je v primeru binomske porazdelitve podatkov *conjugate prior* (pomeni, da apriorna in aposteriorna porazdelitev pripadata enaki druzini porazdelitev), zato se lahko uporablja tudi izraz **beta-binomski model**.

Za apriorno porazdelitev imamo torej gostoto beta porazdelitve pri parametrih $\alpha,\beta>0$:
$$\pi(\theta) = \pi(\theta\mid \alpha, \beta) = \frac{1}{B(\alpha,\beta)} \theta^{\alpha-1} (1-\theta)^{\beta-1},$$
kjer je funkcija beta $B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$ in je funkcija gama $\Gamma(a)=(a-1)!$ za pozitivna cela stevila $a$. Spomnimo se:

* $\text{E}(\text{Beta}(\alpha,\beta))=\frac{\alpha}{\alpha+\beta}$,
  \vspace{0.1cm}
* $\text{var}(\text{Beta}(\alpha,\beta))=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$.

Najprej poskusimo $\alpha=\beta=1$, s cimer dobimo enakomerno zvezno porazdelitev U[0,1] - **neinformativna apriorna porazdelitev**.

Narisemo v R-u:
```{r}
alpha <- 1
beta <- 1

theta <- seq(0, 1, 0.001)
apriorna <- dbeta(theta, alpha, beta)

konst.verjetje <- konst(k, n) * verjetje(theta, k, n)

y.max <- max(c(konst.verjetje, apriorna))
plot(theta, konst.verjetje, ylim = c(0, y.max), type = "l",
     xlab = expression(theta), ylab = "")
lines(theta, apriorna, col = "red")
legend("topright", legend = c("verjetje", "apriorna"), col = c("black", "red"),
       lty = 1, bty = "n", cex = 1.3)
```


## Aposteriorna porazdelitev

Ker smo uporabili *conjugate prior*, bo aposteriorna porazdelitev tudi iz druzine beta porazdelitev, njena parametra sta enaka:

* $\alpha_{\text{apost}} = k+\alpha$,
  \vspace{0.1cm}
* $\beta_{\text{apost}} = n-k+\beta$.

Narisemo v R-u:
```{r}
alpha.apost <- k + alpha
beta.apost <- n - k + beta

theta <- seq(0.001, 1, 0.001)
aposteriorna <- dbeta(theta, alpha.apost, beta.apost)

konst.verjetje <- konst(k, n) * verjetje(theta, k, n)
apriorna <- dbeta(theta, alpha, beta)

y.max <- max(c(konst.verjetje, apriorna, aposteriorna))
plot(theta, konst.verjetje, ylim=c(0, y.max), type = "l",
     xlab = expression(theta), ylab = "")
lines(theta, apriorna, col = "red")
lines(theta, aposteriorna, col = "green3")
legend("topright", legend = c("verjetje", "apriorna","aposteriorna"),
       col = c("black","red","green3"), lty = 1, bty = "n", cex = 1.3)
```

## Ocena parametra $\theta$

Ena moznost je pricakovana vrednost aposteriorne porazdelitve:
$$\hat{\theta} = \frac{\alpha_{\text{apost}}}{\alpha_{\text{apost}}+\beta_{\text{apost}}} = \frac{k+\alpha}{(k+\alpha)+(n-k+\beta)} = \frac{k+\alpha}{n+\alpha+\beta}.$$

```{r}
alpha.apost / (alpha.apost + beta.apost)
```

Ali dobimo enako kakor pri frekventisticnemu pristopu?

```{r}
k/n
```

V primeru neinformativne porazdelitve $\alpha=\beta=1$, dobimo
$$\hat{\theta} = \frac{k+1}{n+2}.$$

Na predavanjih ste $\hat{\theta}$, ocenjen preko pricakovane vrednosti aposteriorne porazdelitve, zapisali kot
$$\hat{\theta} = \frac{\phi}{\phi+n}\cdot\mu+\frac{n}{\phi+n}\cdot\frac{k}{n},$$
kjer je $\mu = \text{E}(\text{Beta}(\alpha,\beta)) = \frac{\alpha}{\alpha+\beta}$ in $\phi = \alpha + \beta.$

**Ideja: $\hat{\theta}$ je utezeno povprecje med $\text{E}(\text{apriorna})$ in $\text{E}(X)$, kjer preko $\phi$ kontroliramo, kako mocno verjamemo apriorni pricakovani vrednosti.**


## Interval (obmocje) zaupanja v Bayesovi statistiki

Pri danih podatkih $X=x$ ima interval $[L_B(x),U_B(x)]$ 95% **Bayesovo pokritje** za $\theta$, ce velja
$$P(L_B(x)<\theta<U_B(x) \mid X=x) = 0,95.$$

\vspace{2cm}

Preden zberemo podatke ima interval $[L(X),U(X)]$ 95% **frekventisticno pokritje** za $\theta$, ce velja
$$P(L(X)<\theta<U(X) \mid \theta) = 0,95.$$

\vspace{2cm}

Ko poznamo podatke $X=x$, jih vstavimo v $L(X)$ in $U(X)$ ter s tem dobimo $L(x)$ in $U(x)$.
$$\text{Koliko je }\,P(L(x)<\theta<U(x) \mid \theta) \; ?$$

\vspace{2cm}

Interval zaupanja v Bayesovi statistiki (angl. pogosto *credible interval*, lahko tudi *confidence interval*) je torej katerikoli interval, v katerem "je vsebovanih 95% gostote aposteriorne porazdelitve". Seveda pa zelimo, da je "centralen glede na porazdelitev".

Najbolj preprosta varianta preko kvantilov porazdelitve (smiselna, ce je porazdelitev priblizno simetricna):
```{r}
(iz <- qbeta(c(0.025,0.975),alpha.apost,beta.apost))
```

\vspace{0.5cm}
```{r}
plot(theta, aposteriorna, type = "l", col="green3",
     xlab = expression(theta), ylab = "")
segments(x0 = iz[1], y0 = 0, 
         x1 = iz[1], y1 = dbeta(iz[1], alpha.apost, beta.apost))
segments(x0 = iz[2], y0 = 0, 
         x1 = iz[2], y1 = dbeta(iz[2], alpha.apost, beta.apost))
legend("topright", legend = c("aposteriorna","IZ preko kvantilov"),
       col = c("green3","black"), lty = 1, bty = "n", cex = 1.3)
```

\vspace{-0.5cm}
*Highest posterior density (HPD) region:*

```{r, warning=F, message=F}
#install.packages("HDInterval")
library(HDInterval)

aposteriorna.sample <- rbeta(100000, alpha.apost, beta.apost)
(iz.hdi <- hdi(aposteriorna.sample, credMass = 0.95))
```

\vspace{-1cm}
```{r, echo=F, warning=F, message=F}
plot(theta, aposteriorna, type = "l", col="green3",
     xlab = expression(theta), ylab = "")
segments(x0 = iz[1], y0 = 0, x1 = iz[1], y1 = dbeta(iz[1], alpha.apost, beta.apost))
segments(x0 = iz[2], y0 = 0, x1 = iz[2], y1 = dbeta(iz[2], alpha.apost, beta.apost))
segments(x0 = iz.hdi[1], y0 = 0, x1 = iz.hdi[1], y1 = dbeta(iz.hdi[1], alpha.apost, beta.apost), col="red")
segments(x0 = iz.hdi[2], y0 = 0, x1 = iz.hdi[2], y1 = dbeta(iz.hdi[2], alpha.apost, beta.apost), col="red")
segments(x0 = iz.hdi[1], y0 = dbeta(iz.hdi[1], alpha.apost, beta.apost), x1 = iz.hdi[2], y1 = dbeta(iz.hdi[1], alpha.apost, beta.apost), col="red")
legend("topright", legend = c("aposteriorna","IZ preko kvantilov","IZ kot HPD region"),
       col = c("green3","black","red"), lty = 1, bty = "n", cex = 1.3)
```


Kaksen interval zaupanja dobimo s frekventisticnim pristopom? Katere intervale zaupanja imamo na voljo?
\vspace{0.3cm}
```{r}
prop.test(k, n, correct=F)$conf #IZ z aproksimacijo normalne porazdelitve
binom.test(k, n)$conf #Clopper-Pearsonov IZ
```

```{r}
iz #Bayesov IZ, metoda s kvantili
```


## Testiranje hipotez

Namen stirih moznih odgovorov je bil, da je verjetnost pravilnega odgovora brez kakrsnegakoli ucenja dovolj majhna. Ali lahko sklepamo, da je verjetnost pravilnega odgovora manjsa od 0,4?

Kako testiramo to hipotezo s Bayesovim pristopom? Kaj mora biti nicelna in kaj alternativna hipoteza?

Kako testiramo s frekventisticnim pristopom?

\newpage

```{r}
#Beayesonski pristop
pbeta(0.4, alpha.apost, beta.apost)

#Test z aproksimacijo normalne porazdelitve
prop.test(k, n, p = 0.4, alternative = "less", correct = FALSE)$p.value

#Binomski eksaktni test
binom.test(k, n, p = 0.4, alternative = "less")$p.value 
```

\newpage

# Primerjava dveh neodvisnih delezev

Nase vprasanje iz zacetka navodil tega sklopa zastavimo se skupini studentov, ki se je ucila.

Od 30 studentov, jih je 21 odgovorilo pravilno.

Verjetnost pravilnega odgovora za studenta, ki se uci, naj bo $\theta_{\text{uci}}$. Ocenimo jo z uporabo neinformativne apriorne porazdelitve (tako kakor prej, $\alpha=\beta=1$). Dobimo aposteriorno porazdelitev
$$\theta_{\text{uci}} \sim \text{Beta}(21+1,30-21+1) = \text{Beta}(22,10).$$
Ocenimo $\hat{\theta}_{\text{uci}}=22/(22+10)=0,6875$.

Kaj smo predpostavili, da smo lahko uporabili ta model?

Prej smo ocenili verjetnost pravilnega odgovora za studenta, ki nakljucno izbere pravilni odgovor, kot
$$\theta_{\text{blef}} \sim \text{Beta}(6+1,26-6+1) = \text{Beta}(7,21).$$
Ocenimo $\hat{\theta}_{\text{blef}}=7/(7+21)=0,25$.

**Zelimo primerjati ti dve verjetnosti.**

Na kaksne nacine ju lahko primerjamo? Katere mere povezanosti lahko izracunamo?

\clearpage

+ Razlika delezev (angl. *risk difference*): $\theta_{\text{uci}} - \theta_{\text{blef}}.$
\vspace{0.3cm}
+ Relativno tveganje (angl. *risk ratio*): $\theta_{\text{uci}}/\theta_{\text{blef}}.$
\vspace{0.3cm}
+ Razmerje obetov (angl. *odds ratio*): $$\frac{\theta_{\text{uci}}/(1-\theta_{\text{uci}})}{\theta_{\text{blef}}/(1-\theta_{\text{blef}})}.$$

Kako ocenimo vsako izmed teh mer?

\vspace{1cm}

Ali znate s orodji frekventisticne statistike testirati, da je razlika delezev vecja od nic?

\vspace{2cm}

Kako bi testirali, da je relativno tveganje vecje od ena (studentje, ki se ucijo, imajo torej vecjo verjetnost pravilnega odgovora)? Kako bi izracunali interval zaupanja za relativno tveganje?

Frekventisticno?

Bayesovsko?

Moramo pri teh dveh pristopih kaj izpeljati?

\newpage

```{r}
uci <- rbeta(1000000, 22, 10)
blef <- rbeta(1000000, 7, 21)

rr <- uci/blef
```

```{r}
hist(rr, prob = TRUE)
axis(1, at = 1)
abline(v = 1, col = "red")
```

\vspace{-0.5cm}
```{r}
## Ocena:
mean(rr)

## Interval zaupanja:
quantile(rr, probs = c(0.025, 0.975)) # preko kvantilov
hdi(aposteriorna.sample, credMass = 0.95) #preko HPD region

## Verjetnost hipoteze, da ucenje pomaga:
sum(rr>1)/length(rr)
```

\clearpage

# Napovedovanje (angl. *prediction*)

Izpit je sestavljen iz desetih vprasanj (taksnih iz zacetka navodil tega sklopa).

1. Denimo, da bi pred zacetkom prvih vaj dali izpit v resevanje nekemu studentu. Kaj lahko povemo o porazdelitvi stevila njegovih pravilnih odgovorov?

2. Na prvih vajah smo pridobili vzorec, s katerim smo preizkusili, kako na vprasanje odgovarjamo, ce ne znamo cisto nic. Vzorec ste bili studentje, prisotni na prvih vajah. Izpit damo v resevanje studentu, **ki ni bil prisoten na prvih vajah** in se tudi ni ucil. Kaj lahko povemo o porazdelitvi stevila njegovih pravilnih odgovorov?

Odgovor na 1. vprasanje je **apriorna napovedna porazdelitev** (angl. *prior predictive distribution*). \newline
Ta nas tipicno ne zanima.

Odgovor na 2. vprasanje je **aposteriorna napovedna porazdelitev** (angl. *posterior predictive distribution*).

**Splosna formula za apriorno napovedno porazdelitev**:
$$f(x_{\text{nov}}) = \int_\Theta f(x_{\text{nov}}, \theta) \, d\theta 
= \int_\Theta f(x_{\text{nov}} \mid \theta)\pi(\theta) \, d\theta.$$

**Splosna formula za aposteriorno napovedno porazdelitev**:
$$f(x_{\text{nov}} \mid x) = \int_\Theta f(x_{\text{nov}}, \theta \mid x) \, d\theta
= \int_\Theta f(x_{\text{nov}} \mid \theta, x)\pi(\theta \mid x) \, d\theta 
= \int_\Theta f(x_{\text{nov}} \mid \theta)\pi(\theta \mid x) \, d\theta.$$

V nasem modelu (binomski model z apriorno beta porazdelitvijo) je:

+ $\pi(\theta) \sim \text{Beta}(\alpha,\beta)$; izbrali smo $\alpha=1, \beta=1$
  \vspace{0.3cm}
+ $\pi(\theta \mid x) \sim \text{Beta}(\alpha_{\text{apost}},\beta_{\text{apost}}) = \text{Beta}(k+\alpha,n-k+\beta)$; za nas vzorec velikosti $n=26$ smo dobili $k=6$
  \vspace{0.3cm}
+ za $x_{\text{nov}} \equiv K \in \{0,1,\ldots,N\}$ je $f(x_{\text{nov}} \mid \theta) = {N \choose K} \theta^{K} (1-\theta)^{N-K}$; dolocili smo $N=10$, zanimajo nas vsi mozni $K$

Izkaze se, da je iskana apriorna ali aposteriorna napovedna porazdelitev iz druzine t.i. **beta-binomske porazdelitve** (BetaBin). To je diskretna porazdelitev $Y$ s parametri $N \in \mathbb{N}$ in $\tilde{\alpha},\tilde{\beta}>0$, ki lahko zavzame vrednosti $K \in \{0,1,\ldots,N\}$ in je
$$P(Y=K) = {N\choose K } \frac{B(K+\tilde{\alpha},N-K+\tilde{\beta})}{B(\tilde{\alpha},\tilde{\beta})}.$$

**Apriorna napovedna porazdelitev v binomskem modelu**: BetaBin($N,\alpha,\beta$).

**Aposteriorna napovedna porazdelitev v binomskem modelu**: BetaBin($N,\alpha_{\text{apost}},\beta_{\text{apost}}$) oziroma  BetaBin($N,k+\alpha,n-k+\beta$).

Beta-binomska porazdelitev v R (je vkljucena tudi v nekaterih paketih, ponekod drugace parametrizirana):
```{r}
dbetabinom <- function(K, N, a, b){
  choose(N, K) * beta(K+a, N-K+b) / beta(a, b)
}
```


Narisemo apriorno napovedno porazdelitev.
```{r}
plot(0:10, dbetabinom(0:10, N = 10, a = alpha, b = beta), type = "h",
     xlab = "Stevilo pravilnih odgovorov", ylab = "verjetnost", 
     main = "Apriorna napoved", ylim = c(0, 0.1), lwd = 5)
```


Narisemo aposteriorno napovedno porazdelitev.
```{r}
plot(0:10, dbetabinom(0:10, N = 10, a = alpha.apost, b = beta.apost), type = "h",
     xlab = "Stevilo pravilnih odgovorov", ylab = "verjetnost", 
     main = "Aposteriorna napoved", lwd = 5)
```


Ali je bilo vse to racunanje res potrebno?

+ Nasa ocena parametra po upostevanju podatkov nasega vzorca je \newline $\hat{\theta}=\alpha_{\text{apost}}/(\alpha_{\text{apost}}+\beta_{\text{apost}})=$ `r alpha.apost / (alpha.apost + beta.apost)`.
  \vspace{0.1cm}
+ Stevilo pravilnih odgovorov je porazdeljeno Bin$(10,\theta)$.
  \vspace{0.1cm}
+ Ali je preprosto aposteriorna porazdelitev kar Bin$(10,\hat{\theta})$?

\vspace{0.8cm}
```{r}
plot(0:10, dbinom(0:10, 10, alpha.apost / (alpha.apost + beta.apost)), type = "h",
     xlab = "Stevilo pravilnih odgovorov", ylab = "verjetnost", 
     main = "Aposteriorna napoved", col = "red", lwd = 5)
segments(x0 = seq(0.1,10.1,1), y0 = rep(0,11),
         x1 = seq(0.1,10.1,1), y1 = dbetabinom(0:10, N = 10, a = alpha.apost, b = beta.apost), 
         lwd = 5, col = "green3")
legend("topright",  lty = 1, lwd = 5,
       c("BetaBin", paste("Bin(10,",round(alpha.apost / (alpha.apost + beta.apost),2),")",sep="")), 
       col = c("green3","red"), bty = "n", cex = 1.3)
```

\clearpage
# Jeffreyeva apriorna porazdelitev

To je **neinformativna** apriorna porazdelitev, ki je proporcionalna $\sqrt{ \det { \mathcal{I}(\theta) }}$. Zanjo je znacilno, da je invariantna glede na razlicne reparametrizacije prostora parametrov.

Pri binomskem modelu je Jeffreyeva apriorna porazdelitev Beta($\alpha=0.5$, $\beta=0.5$).

Pri nasih podatkih dobimo:

```{r, echo=FALSE}
alpha <- 0.5
beta <- 0.5

alpha.apost <- k + alpha
beta.apost <- n - k + beta

theta <- seq(0.001, 0.999, 0.001)
konst.verjetje <- konst(k, n) * verjetje(theta, k, n)
apriorna <- dbeta(theta, alpha, beta)
aposteriorna <- dbeta(theta, alpha.apost, beta.apost)

y.max <- max(c(konst.verjetje, apriorna, aposteriorna))
plot(theta, konst.verjetje, ylim=c(0, y.max), type = "l",
     xlab = expression(theta), ylab = "")
lines(theta, apriorna, col = "red")
lines(theta, aposteriorna, col = "green3")
abline(h = 1, lty = 2, col="red")
legend("topright", legend = c("verjetje", "Jeffreyeva apriorna","aposteriorna",
                              "enakomerna"), 
       col = c("black","red","green3","red"), lty = c(1,1,1,2), bty = "n", cex = 1.3)
```
