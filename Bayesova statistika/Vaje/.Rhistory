knitr::opts_chunk$set(echo = TRUE, message = FALSE, fig.width = 8, fig.height = 5, out.width = "0.8\\textwidth")
x <- c(1.91, 1.94, 1.68, 1.75, 1.81, 1.83, 1.91, 1.95, 1.77, 1.98,
1.81, 1.75, 1.89, 1.89, 1.83, 1.89, 1.99, 1.65, 1.82, 1.65,
1.73, 1.73, 1.88, 1.81, 1.84, 1.83, 1.84, 1.72, 1.91, 1.63)
### Izberemo parametre apriorne porazdelitve
mu.0 <- 1.78
kappa.0 <- 1
sigma2.0 <- 0.1^2
nu.0 <- 1
### Izracunamo parametre aposteriorne porazdelitve
n <- length(x)
kappa.n <- kappa.0 + n
mu.n <- kappa.0/kappa.n * mu.0 + n/kappa.n * mean(x)
nu.n <- nu.0 + n
sigma2.n <- ( nu.0*sigma2.0 + (n-1)*var(x) +
n*kappa.0/kappa.n * (mean(x)-mu.0)^2 ) / nu.n
### Narisemo porazdelitev
# Prvi nacin: s parametrom 1/sigma^2
mu <- seq(1.7, 1.9, 0.0001)
prec2 <- seq(50, 170, 1)
apost <- matrix(NA, nrow = length(mu), ncol = length(prec2))
for (i in 1:length(mu)) {
for (j in 1:length(prec2)) {
apost[i, j] = dnorm(mu[i], mean = mu.n, sd = sqrt(1/(kappa.n*prec2[j]))) *
dgamma(prec2[j], nu.n/2, nu.n*sigma2.n/2)
}
}
par(mfrow = c(1,2))
contour(mu, prec2, apost)
# Drugi nacin: s parametrom sigma^2
#install.packages("invgamma")
library(invgamma)
mu <- seq(1.7, 1.9, 0.0001)
sigma2 <- seq(1/170, 1/50, 0.001)
apost <- matrix(NA, nrow = length(mu), ncol = length(sigma2))
for (i in 1:length(mu)) {
for (j in 1:length(sigma2)) {
apost[i, j] = dnorm(mu[i], mean = mu.n, sd = sqrt(sigma2[j]/kappa.n)) *
dinvgamma(sigma2[j], shape=nu.n/2, rate=nu.n*sigma2.n/2)
}
}
contour(mu,sigma2,apost)
par(mfrow=c(1,1))
### Izberemo parametre apriorne porazdelitve
mu.0 <- 1.78
kappa.0 <- 1
sigma2.0 <- 0.1^2
nu.0 <- 1
### Izracunamo parametre aposteriorne porazdelitve
n <- length(x)
kappa.n <- kappa.0 + n
mu.n <- kappa.0/kappa.n * mu.0 + n/kappa.n * mean(x)
nu.n <- nu.0 + n
sigma2.n <- ( nu.0*sigma2.0 + (n-1)*var(x) +
n*kappa.0/kappa.n * (mean(x)-mu.0)^2 ) / nu.n
### Narisemo porazdelitev
# Prvi nacin: s parametrom 1/sigma^2
mu <- seq(1.7, 1.9, 0.0001)
prec2 <- seq(50, 170, 1)
apost <- matrix(NA, nrow = length(mu), ncol = length(prec2))
for (i in 1:length(mu)) {
for (j in 1:length(prec2)) {
apost[i, j] = dnorm(mu[i], mean = mu.n, sd = sqrt(1/(kappa.n*prec2[j]))) *
dgamma(prec2[j], nu.n/2, nu.n*sigma2.n/2)
}
}
par(mfrow = c(1,2))
contour(mu, prec2, apost)
# Drugi nacin: s parametrom sigma^2
#install.packages("invgamma")
library(invgamma)
mu <- seq(1.7, 1.9, 0.0001)
sigma2 <- seq(1/170, 1/50, 0.001)
apost <- matrix(NA, nrow = length(mu), ncol = length(sigma2))
for (i in 1:length(mu)) {
for (j in 1:length(sigma2)) {
apost[i, j] = dnorm(mu[i], mean = mu.n, sd = sqrt(sigma2[j]/kappa.n)) *
dinvgamma(sigma2[j], shape=nu.n/2, rate=nu.n*sigma2.n/2)
}
}
contour(mu,sigma2,apost)
par(mfrow=c(1,1))
kappa.n
mu.n
kappa.0/kappa.n
n/kappa.n * mean(x)
nu.n
sigma2.n
mu
prec2
apost
View(apost)
length(prec2)
?contour
mu <- seq(1.7, 1.9, 0.0001)
prec2 <- seq(50, 170, 1)
apost <- matrix(NA, nrow = length(mu), ncol = length(prec2))
for (i in 1:length(mu)) {
for (j in 1:length(prec2)) {
apost[i, j] = dnorm(mu[i], mean = mu.n, sd = sqrt(1/(kappa.n*prec2[j]))) *
dgamma(prec2[j], nu.n/2, nu.n*sigma2.n/2)
}
}
par(mfrow = c(1,2))
contour(mu, prec2, apost)
mu <- seq(1.7, 1.9, 0.0001)
sigma2 <- seq(1/170, 1/50, 0.001)
apost <- matrix(NA, nrow = length(mu), ncol = length(sigma2))
for (i in 1:length(mu)) {
for (j in 1:length(sigma2)) {
apost[i, j] = dnorm(mu[i], mean = mu.n, sd = sqrt(sigma2[j]/kappa.n)) *
dinvgamma(sigma2[j], shape=nu.n/2, rate=nu.n*sigma2.n/2)
}
}
contour(mu,sigma2,apost)
par(mfrow=c(1,1))
mu <- seq(1.7, 1.9, 0.0001)
aposteriorna <- dt((mu-mu.n)/sqrt(sigma2.n/kappa.n), df = nu.n)/sqrt(sigma2.n/kappa.n)
plot(mu, aposteriorna, type="l", main="Robna aposteriorna porazdelitev",
xlab = expression(mu), ylab = "")
source("sportnice.R")
str(sportnice)
fit.lm <- lm(trening ~ kontrace, data = sportnice)
summary(fit.lm)
fit2.lm <- lm(trening ~ kontrace + starost, data = sportnice)
summary(fit2.lm)
fit.bayesx <- bayesx(trening ~ kontrace, data = sportnice, family = "gaussian", method = "MCMC")
summary(fit.bayesx)
fit2.bayesx <- bayesx(trening ~ kontrace + starost, data = sportnice, family = "gaussian", method = "MCMC")
summary(fit2.bayesx)
b.kontrace <- attr(fit2.bayesx$fixed.effects, "sample")[,2]
b.starost <- attr(fit2.bayesx$fixed.effects, "sample")[,3]
fit.bayesx <- bayesx(trening ~ kontrace, data = sportnice, family = "gaussian", method = "MCMC")
#install.packages("R2BayesX")
library(R2BayesX)
library(nimble)
fit.lm <- lm(trening ~ kontrace, data = sportnice)
summary(fit.lm)
fit2.lm <- lm(trening ~ kontrace + starost, data = sportnice)
summary(fit2.lm)
fit.lm <- lm(trening ~ kontrace, data = sportnice)
summary(fit.lm)
fit2.lm <- lm(trening ~ kontrace + starost, data = sportnice)
summary(fit2.lm)
fit.bayesx <- bayesx(trening ~ kontrace, data = sportnice, family = "gaussian", method = "MCMC")
summary(fit.bayesx)
fit2.bayesx <- bayesx(trening ~ kontrace + starost, data = sportnice, family = "gaussian", method = "MCMC")
summary(fit2.bayesx)
b.kontrace <- attr(fit2.bayesx$fixed.effects, "sample")[,2]
b.starost <- attr(fit2.bayesx$fixed.effects, "sample")[,3]
code <- nimbleCode({
beta0 ~ dnorm(0, sd = 100) #tipicno vzamemo taksen neinformativen prior
beta1 ~ dnorm(0, sd = 100)
beta2 ~ dnorm(0, sd = 100)
sigma ~ dunif(0, 100)
for(i in 1:n) {
y[i] ~ dnorm(beta0 + beta1*x1[i] + beta2*x2[i], sd = sigma)
}
})
constants <- list(n = length(sportnice$trening),
x1 = sportnice$kontrace,
x2 = sportnice$starost)
data <- list(y = sportnice$trening)
inits <- list(beta0 = mean(sportnice$trening),
beta1 = 0,
beta2 = 0,
sigma = 1)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printMonitors()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Cmodel)
samples <- runMCMC(Cmcmc, niter = 1000)
samples <- runMCMC(Cmcmc, niter = 12000, nburnin = 2000)
dim(samples)
head(samples)
samplesSummary(samples)
summary(fit2.bayesx)
# Model brez interakcije
fit.lm <- lm(trening ~ sport + starost, data = sportnice)
summary(fit.lm)
library(ggplot2)
ggplot(sportnice, aes(x = starost, y = trening, color = sport)) +
geom_point()
ggplot(sportnice, aes(x = starost, y = trening, color = sport)) +
geom_point() +
geom_smooth(method = "lm")
fit2.lm <- lm(trening ~ sport * starost, data = sportnice)
#fit2.lm <- lm(trening ~ sport + starost + sport:starost, data = sportnice) ### ekvivalentno
summary(fit2.lm)
code <- nimbleCode({
beta0 ~ dnorm(0, sd = 100)
for(k in 1:p) { #lahko uporabimo for zanko za definicijo vseh apriornih porazdelitev
beta[k] ~ dnorm(0, sd = 100)
}
sigma ~ dunif(0, 100)
for(i in 1:n) {
y[i] ~ dnorm(beta0 + inprod(beta[1:p], x[i, 1:p]), sd = sigma) #uporabimo funkcijo inprod
}
})
sportnice$odbojka <- ifelse(sportnice$sport=="odbojka", 1, 0) #naredimo slamnate spremenljivke
sportnice$rokomet <- ifelse(sportnice$sport=="rokomet", 1, 0)
sportnice$odbojkaStarost <- sportnice$odbojka * sportnice$starost #spremenljivke za interakcijo
sportnice$rokometStarost <- sportnice$rokomet * sportnice$starost
X <- subset(sportnice, select = c("odbojka",
"rokomet",
"starost",
"odbojkaStarost",
"rokometStarost"))
p <- ncol(X)
constants <- list(n = length(sportnice$trening),
p = p,
x = X)
data <- list(y = sportnice$trening)
inits <- list(beta0 = mean(sportnice$trening),
beta = rep(0, p),
sigma = 1)
p
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
conf$printMonitors()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Cmodel)
samples <- runMCMC(Cmcmc, niter = 1000)
samples <- runMCMC(Cmcmc, niter = 12000, nburnin = 2000)
dim(samples)
head(samples)
summary(fit2.lm)
samplesSummary(samples)
library(basicMCMCplots)
samplesPlot(samples)
samplesPlot(samples, var = "beta[1]")
samplesPlot(samples, var = "beta[2]")
samplesPlot(samples, var = "beta[3]")
initsFunction <- function(){
list(beta0 = rnorm(1),
beta = rnorm(5),
sigma = runif(1, min = 0, max = 10))
}
initsFunction()
samplesList <- runMCMC(Cmcmc, niter = 12000, nburnin = 0,
nchains = 3, inits = initsFunction)
st
str(samplesList)
str(samplesList$chain1) #dobimo eno verigo
str(samplesList[[1]])   #ekvivalentno
samplesPlot(samplesList[[1]], var = "beta[2]") #si narisemo 1 parameter iz 1 verige
# Namesto, da bi ze zgoraj dolocili burnin, ga lahko tule:
samplesPlot(samplesList[[1]], var = "beta[2]", burnin = 2000)
chainsPlot(samplesList) #dobimo trace plots in gostote aposteriornih porazdelitev
# Preprosto vzamemo nek burnin:
chainsPlot(samplesList, burnin = 2000)
chainsSummary(samplesList) #dobimo mediane in credible intervals
chainsSummary(samplesList, scale = TRUE) #normaliziramo, tako da vse na isti skali
chainsSummary(samplesList, scale = TRUE) #normaliziramo, tako da vse na isti skali
# V tem ukazu ni parametra burnin, zato to naredimo rocno:
samplesList2 <- samplesList
for (i in 1:3) samplesList2[[i]] <- samplesList2[[i]][2001:12000,]
chainsSummary(samplesList2, scale = TRUE)
fit2.bayesx <- bayesx(trening ~ kontrace + starost, data = sportnice, family = "gaussian", method = "MCMC")
summary(fit2.bayesx)
b.kontrace <- attr(fit2.bayesx$fixed.effects, "sample")[,2]
b.starost <- attr(fit2.bayesx$fixed.effects, "sample")[,3]
par(mfrow = c(2, 2))
plot(b.kontrace, type = "l", main = "Koeficient za kontracepcijo,\nveriga",
xlab = "")
plot(b.starost, type = "l", main = "Koeficient za starost,\nveriga",
xlab = "")
hist(b.kontrace, prob = T, main = "Koeficient za kontracepcijo,\nrobna aposteriorna porazdelitev")
lines(density(b.kontrace), col = "red", lwd = 2)
hist(b.starost, prob = T, main = "Koeficient za starost, \nrobna aposteriorna porazdelitev")
lines(density(b.starost), col = "red", lwd = 2)
par(mfrow = c(1, 1))
library(coda)
effectiveSize(b.kontrace) #1000
effectiveSize(b.starost)  #1000
# Poskusimo, kaj se zgodi brez thinninga:
fit2.bayesx <- bayesx(trening ~ kontrace + starost, data = sportnice, family = "gaussian", method = "MCMC",
step = 1)
b.kontrace <- attr(fit2.bayesx$fixed.effects, "sample")[,2]
b.starost <- attr(fit2.bayesx$fixed.effects, "sample")[,3]
effectiveSize(b.kontrace) #10000, kar je najvecje mozno
effectiveSize(b.starost)  #10000, kar je najvecje mozno
code <- nimbleCode({
beta0 ~ dnorm(0, sd = 100)
beta1 ~ dnorm(0, sd = 100)
beta2 ~ dnorm(0, sd = 100)
sigma ~ dunif(0, 100)
for(i in 1:n) {
y[i] ~ dnorm(beta0 + beta1*x1[i] + beta2*x2[i], sd = sigma)
}
})
constants <- list(n = length(sportnice$trening),
x1 = as.numeric(sportnice$kontrace),
x2 = sportnice$starost)
data <- list(y = sportnice$trening)
inits <- list(beta0 = mean(sportnice$trening),
beta1 = 0,
beta2 = 0,
sigma = 1)
Rmodel <- nimbleModel(code, constants, data, inits)
conf <- configureMCMC(Rmodel)
conf$printSamplers()
Rmcmc <- buildMCMC(conf)
Cmodel <- compileNimble(Rmodel)
Cmcmc <- compileNimble(Rmcmc, project = Cmodel)
samples <- runMCMC(Cmcmc, niter = 12000, nburnin = 2000)
samplesSummary(samples)
samplesPlot(samples)
effectiveSize(samples) #obupno majhni
cor(samples) #razlog je koreliranost koeficientov, resitev je centriranje
constants.centr <- list(n = length(sportnice$trening),
x1 = as.numeric(sportnice$kontrace)-mean(as.numeric(sportnice$kontrace)),
x2 = sportnice$starost-mean(sportnice$starost))
Rmodel.centr <- nimbleModel(code, constants.centr, data, inits)
conf.centr <- configureMCMC(Rmodel.centr)
Rmcmc.centr <- buildMCMC(conf.centr)
Cmodel.centr <- compileNimble(Rmodel.centr)
Cmcmc.centr <- compileNimble(Rmcmc.centr, project = Cmodel.centr)
samples.centr <- runMCMC(Cmcmc.centr, niter = 12000, nburnin = 2000)
